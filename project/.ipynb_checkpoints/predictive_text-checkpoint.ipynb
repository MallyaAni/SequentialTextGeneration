{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "import enchant\n",
    "import pickle\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.lm import Vocabulary\n",
    "from nltk.tag import StanfordNERTagger, pos_tag\n",
    "from sklearn.model_selection import learning_curve\n",
    "import glob\n",
    "import contractions\n",
    "import string\n",
    "import bs4 as bs\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import urllib.request\n",
    "import re\n",
    "import multiprocessing\n",
    "import ndjson\n",
    "from gensim.models import Word2Vec\n",
    "import ssl\n",
    "import getch\n",
    "from google.cloud import storage\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.models import model_from_json, Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Embedding, Flatten, Activation, Bidirectional, LSTM, GRU\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from tensorflow.python.keras.metrics import categorical_accuracy\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "\n",
    "%load_ext autotime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(7)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "import os\n",
    "\n",
    "savepath = '/Users/animallya/Desktop/NLP_project/project/SeekGen_flask_app/SeekGen/savefiles/'\n",
    "embedpath = '/Users/animallya/Desktop/NLP_project/project/SeekGen_flask_app/SeekGen/embeddings/'\n",
    "\n",
    "ngram_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def filter_words(text, global_vocab):\n",
    "    words = []\n",
    "    for w in text:\n",
    "        w = w.lower()\n",
    "        if len(w)>1:\n",
    "            if global_vocab.check(w)==True:            \n",
    "                if '\\'' in w or '’' in w:\n",
    "                    if contractions.fix(w) != w:\n",
    "                        fixedw = contractions.fix(w).lower()\n",
    "                        words.append(fixedw)\n",
    "                else:\n",
    "                    words.append(w)\n",
    "        else:\n",
    "            if w in['i','a'] or w in string.punctuation:\n",
    "                words.append(w)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def message_scrape(path, ids):\n",
    "    \n",
    "    conn = sqlite3.connect(path)\n",
    "    c = conn.cursor()\n",
    "    output = \"\"\n",
    "    \n",
    "    for n in range(ids):\n",
    "        cmd1 = 'SELECT ROWID, text, handle_id, \\\n",
    "                    datetime(date + strftime(\\'%s\\',\\'2001-01-01\\'), \\'unixepoch\\') as date_utc \\\n",
    "                    FROM message T1 \\\n",
    "                    INNER JOIN chat_message_join T2 \\\n",
    "                        ON T2.chat_id='+str(n)+ ' \\\n",
    "                        AND T1.ROWID=T2.message_id \\\n",
    "                    ORDER BY T1.date'\n",
    "        c.execute(cmd1)\n",
    "        df_msg = pd.DataFrame(c.fetchall(), columns=['id', 'text', 'sender', 'time'])\n",
    "        corpus = df_msg.text.tolist()\n",
    "        output +=' '+' '.join(corpus)\n",
    "\n",
    "    corpus = re.sub('[^a-zA-Z\\'\\’\\?]', ' ', output)\n",
    "    corpus = re.sub(r'\\s+', ' ', corpus)\n",
    "    corpus = corpus.lower().split(\" \")\n",
    "    filtered_corpus = filter_words(corpus, enchant.Dict(\"en_US\"))\n",
    "    '''\n",
    "    frequency filtering, use nltk library to get frequency of all unique tokens in corpus and remove all of those that are\n",
    "    above some threshold.(research what threshold you wana use?)\n",
    "    When you lemmatize a word, store its original form as the value to the root form of the word. Root form is the key in a\n",
    "    dictionary. Ex: {'go':'went','going'} and then we can use that dictionary to de-lemmatize it during prediction.\n",
    "    '''\n",
    "    return filtered_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram Neural Model predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate glove embeddings savefile, uncomment next 3 lines if running for first time.\n",
    "# glove_input_file = embedpath+'glove.42B.300d.txt'\n",
    "word2vec_output_file = embedpath+'glove.42B.300d.txt.word2vec'\n",
    "# glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "embeddings = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    corpus = message_scrape(path=savepath+'chat1.db',ids=100)+' '+ message_scrape(path=savepath+'chat2.db',ids=100)\n",
    "    words = nltk.word_tokenize(corpus)\n",
    "    #automate this later\n",
    "    words = list(filter(lambda a: a != 'bet', words))\n",
    "    vocab = list(Vocabulary(words))\n",
    "    return vocab, words, corpus\n",
    "\n",
    "def generate_traindata(vocab, words):\n",
    "    #Y = [vocab.index(words[i]) for i in range(ngram_size, len(words))]\n",
    "    Y = [embeddings[word] for i in range(ngram_size, len(words))]\n",
    "    X = [np.array([embeddings[word] for word in words[i-ngram_size:i]]) for i in range(ngram_size, len(words))]\n",
    "    Y = [to_categorical(y,len(vocab)) for y in Y]\n",
    "    X, Y = shuffle(X, Y, random_state=42)\n",
    "    return np.array(X),np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, words, corpus = preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = generate_traindata(vocab, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectional_lstm_model(X):\n",
    "    inputs = keras.Input(shape=(X.shape[1], X.shape[2]), name='digits')\n",
    "    x = keras.Sequential()(inputs)\n",
    "    x = Bidirectional(LSTM(512,activation=\"tanh\"))(x)\n",
    "    outputs = Dense(len(vocab),activation='softmax')(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='bdrnn')\n",
    "    print(\"model built!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = bidirectional_lstm_model(X)\n",
    "rnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_rnn = rnn_model.fit(X, y, batch_size=128, epochs= 25, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_rnn.history['val_categorical_accuracy'])\n",
    "plt.title('validation loss')\n",
    "plt.ylabel('val loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pos-tagging using SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "file = nltk.corpus.webtext.fileids()[-1] #ID is the last one\n",
    "#webtext = ' '.join(nltk.corpus.reuters.words(file))\n",
    "doc = nlp(corpus)\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "xtemp = [token.tag_ for token in doc]\n",
    "encoded = label_encoder.fit_transform(xtemp)\n",
    "X_encoded = [encoded[i-ngram_size:i] for i in range(ngram_size,len(encoded))]\n",
    "y_encoded = [encoded[i] for i in range(ngram_size,len(encoded))]\n",
    "y_encoded = np.array(y_encoded)\n",
    "X_encoded = np.array(X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_model = SVC(gamma='auto',probability=True, class_weight='balanced',verbose=2)\n",
    "tag_model.fit(X_encoded, y_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving all 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/Users/animallya/Desktop/NLP_project/project/SeekGen_flask_app/SeekGen/savefiles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_name):\n",
    "    model.save(save_path+model_name+'.h5') \n",
    "\n",
    "#save_model(rnn_model,'rnn_model')\n",
    "\n",
    "# filename = save_path+'postagger.sav'\n",
    "# pickle.dump(tag_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = tf.keras.models.load_model(save_path+'rnn_model.h5')\n",
    "tag_model = pickle.load(open(save_path+'postagger.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_tag_to_prediction(inp):\n",
    "    doc = nlp(inp.lower())\n",
    "    tags = [token.tag_ for token in doc]\n",
    "    X_encoded = label_encoder.transform(tags)\n",
    "    y_pred = tag_model.predict_proba(X_encoded.reshape(1, -1))[0]\n",
    "    indices = (y_pred).argsort()[::-1][:3]\n",
    "    return [label_encoder.inverse_transform([idx])[0] for idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_word_to_prediction(inp):\n",
    "    words = nltk.word_tokenize(inp.lower())\n",
    "    words_tf = np.array([embeddings[word] for word in words]).reshape(1, -1)\n",
    "    y_pred = sequence_model.predict_proba(words_tf)[0]\n",
    "    indices = (y_pred).argsort()[::-1][:3]\n",
    "    return [vocab[idx] for idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_predict(sent):\n",
    "    words = nltk.word_tokenize(sent.lower())\n",
    "    x = np.array([embeddings[word] for word in words])\n",
    "    words_tf = x.reshape(1,x.shape[0],x.shape[1])\n",
    "    y_pred = np.array(rnn_model.predict(words_tf)[0])\n",
    "    indices = (y_pred).argsort()[::-1][:3]\n",
    "    return [vocab[idx] for idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_union(sent):\n",
    "    tags = input_tag_to_prediction(sent)\n",
    "    superset_tags = [tag[:2] for tag in tags]\n",
    "    predictions = rnn_predict(sent)\n",
    "    prediction_tags = [nlp(word)[0].tag_[:2] for word in predictions]\n",
    "    filtered_predictions = [word for word in predictions if nlp(word)[0].tag_[:2] in superset_tags]\n",
    "    return predictions, filtered_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(sent):\n",
    "    out = sent\n",
    "    while True:\n",
    "        print(out)\n",
    "        predictions, filtered_predictions = prediction_union(sent)\n",
    "        print(predictions)\n",
    "        print(filtered_predictions)\n",
    "        inp = input()\n",
    "        if inp==\"exit\":\n",
    "            return out\n",
    "        out+= \" \" +inp\n",
    "        l = len(inp.split())\n",
    "        sent = \" \".join(nltk.word_tokenize(sent)[l:]+[inp])\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = run(\"oh yeah what\")\n",
    "#i will be home by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enchant.Dict(\"en_US\").suggest('sx')\n",
    "embeddings.most_similar(positive=['king','female'], negative=['male'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.most_similar('way')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.read_csv('SeekGen_flask_app/SeekGen/eval/evaluation.csv')\n",
    "df_eval['Precision'] = df_eval['Pred_used'].divide(df_eval['Num_words'])\n",
    "df_eval = df_eval.sort_values(by=['Precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.Precision.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
