{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Ani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.98 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.lm import Vocabulary\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import pickle\n",
    "import nltk\n",
    "import ssl\n",
    "%load_ext autotime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 3659747947308028782\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10189863649\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 5578378573520185249\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:3d:00.0, compute capability: 5.2\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map tags to meanings and examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 22.9 ms\n"
     ]
    }
   ],
   "source": [
    "tagmap = {\n",
    "'CC': 'coordinating conjunction',\n",
    "'CD': 'cardinal digit',\n",
    "'DT': 'determiner',\n",
    "'EX': 'existential there (like: “there is”)',\n",
    "'FW': 'foreign word',\n",
    "'IN': 'preposition/subordinating conjunction',\n",
    "'JJ' :'adjective ‘big’',\n",
    "'JJR' :'adjective, comparative ‘bigger’',\n",
    "'JJS': 'adjective, superlative ‘biggest’',\n",
    "'LS': 'list marker 1)',\n",
    "'MD': 'modal, could, will',\n",
    "'NN': 'noun, singular ex:‘desk',\n",
    "'NNS': 'noun plural ex:‘desks',\n",
    "'NNP': 'proper noun, singular ex:‘Harrison’',\n",
    "'NNPS':'proper noun, plural ex:‘Americans’',\n",
    "'PDT': 'predeterminer ex:‘all the kids’',\n",
    "'POS': 'possessive ending ex:parent’s',\n",
    "'PRP': 'personal pronoun ex:I, he, she',\n",
    "'PRP$':'possessive pronoun ex:my, his, hers',\n",
    "'RB': 'adverb ex:very, silently,',\n",
    "'RBR': 'adverb, comparative ex:better',\n",
    "'RBS': 'adverb, superlative ex:best',\n",
    "'RP': 'particle give up',\n",
    "'TO': 'to go ex:‘to’ the store.',\n",
    "'UH': 'interjection, ex:errrrrrrrm',\n",
    "'VB': 'verb, base form ex:take',\n",
    "'VBD': 'verb, past tense ex:took',\n",
    "'VBG': 'verb, gerund/present participle ex:taking',\n",
    "'VBN': 'verb, past participle ex:taken',\n",
    "'VBP': 'verb, sing. present, non-3d ex:take',\n",
    "'VBZ': 'verb, 3rd person sing. present ex:takes',\n",
    "'WDT': 'wh-determiner ex:which',\n",
    "'WP': 'wh-pronoun ex:who, what',\n",
    "'WP$': 'possessive wh-pronoun ex:whose',\n",
    "'WRB': 'wh-abverb ex:where, when'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping data about AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.29 s\n"
     ]
    }
   ],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "scrapped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Artificial_intelligence')\n",
    "article = scrapped_data .read()\n",
    "\n",
    "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "\n",
    "paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "article_text = \"\"\n",
    "\n",
    "for p in paragraphs:\n",
    "    article_text += p.text\n",
    "\n",
    "corpus = article_text.lower()\n",
    "corpus = re.sub('[^a-zA-Z]', ' ', corpus)\n",
    "corpus = re.sub(r'\\s+', ' ', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.02 ms\n"
     ]
    }
   ],
   "source": [
    "ngram_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf trigram prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 114 ms\n"
     ]
    }
   ],
   "source": [
    "words = nltk.word_tokenize(corpus)\n",
    "vocab = Vocabulary(words)\n",
    "vectorizer = TfidfVectorizer(vocabulary=vocab)\n",
    "X = vectorizer.fit_transform([corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.98 ms\n"
     ]
    }
   ],
   "source": [
    "feats = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.99 ms\n"
     ]
    }
   ],
   "source": [
    "def preprocess(words, deep_learning=True):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for i in range(ngram_size,len(words)):\n",
    "        if words[i] in feats:\n",
    "            x = np.array(vectorizer.transform(words[i-ngram_size:i]).toarray())\n",
    "            y = feats.index(words[i])\n",
    "            if deep_learning == False:\n",
    "                x = x.flatten()\n",
    "                #maxent model allows for label encoded targets\n",
    "            else:\n",
    "                y = keras.utils.to_categorical(y,len(feats))\n",
    "                #CNN wants categorical targets\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "    return np.array(X),np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.79 s\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = preprocess(words, deep_learning=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13158, 9213)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.99 ms\n"
     ]
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed: 81.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=-1, penalty='l2', random_state=None, solver='newton-cg',\n",
       "          tol=0.0001, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1h 21min 38s\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "count_model = linear_model.LogisticRegression(multi_class='multinomial',solver='newton-cg',n_jobs=-1,verbose=2)\n",
    "count_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pos-tagging using trigram max-ent model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 822 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#encoding sets of 3 tags into flattened vector that's one-hot encoded\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "xtemp = [tag[1] for tag in nltk.pos_tag(words)]\n",
    "int_encoded = label_encoder.fit_transform(xtemp)\n",
    "encoded = onehot_encoder.fit_transform(int_encoded.reshape(len(int_encoded), 1))\n",
    "\n",
    "X_encoded = []\n",
    "y_encoded = []\n",
    "\n",
    "for i in range(ngram_size,len(encoded)):\n",
    "    x_encoded = encoded[i-ngram_size:i].flatten().astype(int)\n",
    "    X_encoded.append(x_encoded)\n",
    "    y_encoded.append(int_encoded[i])\n",
    "    \n",
    "y_encoded = np.array(y_encoded)\n",
    "X_encoded = np.array(X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13158,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.01 ms\n"
     ]
    }
   ],
   "source": [
    "y_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   13.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=-1, penalty='l2', random_state=None, solver='newton-cg',\n",
       "          tol=0.0001, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 14.2 s\n"
     ]
    }
   ],
   "source": [
    "tag_model = linear_model.LogisticRegression(multi_class='multinomial',solver='newton-cg',n_jobs=-1,verbose=2)\n",
    "tag_model.fit(X_encoded, y_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-Deep Learning model semantic understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.08 s\n"
     ]
    }
   ],
   "source": [
    "X_train,y_train = preprocess(words, deep_learning=True)\n",
    "X_train = X_train.reshape(X_train.shape[0],X_train.shape[1],X_train.shape[2],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13158, 3, 3071, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.99 ms\n"
     ]
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1018 08:59:31.901607 23536 deprecation_wrapper.py:119] From c:\\users\\ani\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1018 08:59:31.953477 23536 deprecation_wrapper.py:119] From c:\\users\\ani\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1018 08:59:31.973415 23536 deprecation_wrapper.py:119] From c:\\users\\ani\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1018 08:59:32.051206 23536 deprecation_wrapper.py:119] From c:\\users\\ani\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1018 08:59:32.061180 23536 deprecation.py:506] From c:\\users\\ani\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1018 08:59:32.168893 23536 deprecation_wrapper.py:119] From c:\\users\\ani\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1018 08:59:32.177869 23536 deprecation_wrapper.py:119] From c:\\users\\ani\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 306 ms\n"
     ]
    }
   ],
   "source": [
    "cnn_model = Sequential()\n",
    "cnn_model.add(Conv2D(32, kernel_size=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=(X_train.shape[1],X_train.shape[2],X_train.shape[3])))\n",
    "cnn_model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "#cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#cnn_model.add(Dropout(0.25))\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(128, activation='relu'))\n",
    "cnn_model.add(Dropout(0.5))\n",
    "cnn_model.add(Dense(len(feats), activation='softmax'))\n",
    "cnn_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1018 08:59:34.096933 23536 deprecation.py:323] From c:\\users\\ani\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "13158/13158 [==============================] - 15s 1ms/step - loss: 7.1005 - acc: 0.0366\n",
      "Epoch 2/50\n",
      "13158/13158 [==============================] - 9s 659us/step - loss: 6.7609 - acc: 0.0460\n",
      "Epoch 3/50\n",
      "13158/13158 [==============================] - 9s 659us/step - loss: 6.6947 - acc: 0.0480\n",
      "Epoch 4/50\n",
      "13158/13158 [==============================] - 9s 656us/step - loss: 6.5345 - acc: 0.0694\n",
      "Epoch 5/50\n",
      "13158/13158 [==============================] - 9s 655us/step - loss: 6.2619 - acc: 0.1061\n",
      "Epoch 6/50\n",
      "13158/13158 [==============================] - 9s 654us/step - loss: 5.8920 - acc: 0.1441\n",
      "Epoch 7/50\n",
      "13158/13158 [==============================] - 9s 654us/step - loss: 5.3847 - acc: 0.1872\n",
      "Epoch 8/50\n",
      "13158/13158 [==============================] - 9s 653us/step - loss: 4.7964 - acc: 0.2443\n",
      "Epoch 9/50\n",
      "13158/13158 [==============================] - 9s 662us/step - loss: 4.1769 - acc: 0.2965\n",
      "Epoch 10/50\n",
      "13158/13158 [==============================] - 9s 654us/step - loss: 3.6603 - acc: 0.3480\n",
      "Epoch 11/50\n",
      "13158/13158 [==============================] - 9s 654us/step - loss: 3.2039 - acc: 0.4017\n",
      "Epoch 12/50\n",
      "13158/13158 [==============================] - 9s 656us/step - loss: 2.8306 - acc: 0.4469\n",
      "Epoch 13/50\n",
      "13158/13158 [==============================] - 9s 657us/step - loss: 2.4948 - acc: 0.4958\n",
      "Epoch 14/50\n",
      "13158/13158 [==============================] - 9s 656us/step - loss: 2.2820 - acc: 0.5303\n",
      "Epoch 15/50\n",
      "13158/13158 [==============================] - 9s 654us/step - loss: 2.0555 - acc: 0.5670\n",
      "Epoch 16/50\n",
      "13158/13158 [==============================] - 9s 655us/step - loss: 1.8931 - acc: 0.5920\n",
      "Epoch 17/50\n",
      "13158/13158 [==============================] - 9s 655us/step - loss: 1.7531 - acc: 0.6193\n",
      "Epoch 18/50\n",
      "13158/13158 [==============================] - 9s 656us/step - loss: 1.6431 - acc: 0.6332\n",
      "Epoch 19/50\n",
      "13158/13158 [==============================] - 9s 661us/step - loss: 1.5498 - acc: 0.6512\n",
      "Epoch 20/50\n",
      "13158/13158 [==============================] - 9s 657us/step - loss: 1.4503 - acc: 0.6673\n",
      "Epoch 21/50\n",
      "13158/13158 [==============================] - 9s 655us/step - loss: 1.3774 - acc: 0.6860\n",
      "Epoch 22/50\n",
      "13158/13158 [==============================] - 9s 653us/step - loss: 1.3383 - acc: 0.6916\n",
      "Epoch 23/50\n",
      "13158/13158 [==============================] - 9s 672us/step - loss: 1.2661 - acc: 0.7043\n",
      "Epoch 24/50\n",
      "13158/13158 [==============================] - 9s 656us/step - loss: 1.2427 - acc: 0.7073\n",
      "Epoch 25/50\n",
      "13158/13158 [==============================] - 9s 665us/step - loss: 1.2133 - acc: 0.7182\n",
      "Epoch 26/50\n",
      "13158/13158 [==============================] - 9s 661us/step - loss: 1.1548 - acc: 0.7248\n",
      "Epoch 27/50\n",
      "13158/13158 [==============================] - 9s 653us/step - loss: 1.1265 - acc: 0.7294\n",
      "Epoch 28/50\n",
      "13158/13158 [==============================] - 9s 654us/step - loss: 1.1310 - acc: 0.7317\n",
      "Epoch 29/50\n",
      "13158/13158 [==============================] - 9s 655us/step - loss: 1.0803 - acc: 0.7390\n",
      "Epoch 30/50\n",
      "13158/13158 [==============================] - 9s 663us/step - loss: 1.0559 - acc: 0.7443\n",
      "Epoch 31/50\n",
      "13158/13158 [==============================] - 9s 653us/step - loss: 1.0644 - acc: 0.7433\n",
      "Epoch 32/50\n",
      "13158/13158 [==============================] - 9s 655us/step - loss: 1.0193 - acc: 0.7491\n",
      "Epoch 33/50\n",
      "13158/13158 [==============================] - 9s 668us/step - loss: 0.9931 - acc: 0.7585\n",
      "Epoch 34/50\n",
      "13158/13158 [==============================] - 9s 664us/step - loss: 0.9910 - acc: 0.7532\n",
      "Epoch 35/50\n",
      "13158/13158 [==============================] - 9s 656us/step - loss: 0.9941 - acc: 0.7532\n",
      "Epoch 36/50\n",
      "13158/13158 [==============================] - 9s 656us/step - loss: 0.9564 - acc: 0.7614\n",
      "Epoch 37/50\n",
      "13158/13158 [==============================] - 9s 657us/step - loss: 0.9354 - acc: 0.7673\n",
      "Epoch 38/50\n",
      "13158/13158 [==============================] - 9s 681us/step - loss: 0.9353 - acc: 0.7617\n",
      "Epoch 39/50\n",
      "13158/13158 [==============================] - 9s 654us/step - loss: 0.9308 - acc: 0.7665\n",
      "Epoch 40/50\n",
      "13158/13158 [==============================] - 9s 655us/step - loss: 0.9374 - acc: 0.7666\n",
      "Epoch 41/50\n",
      "13158/13158 [==============================] - 9s 653us/step - loss: 0.9026 - acc: 0.7751\n",
      "Epoch 42/50\n",
      "13158/13158 [==============================] - 9s 654us/step - loss: 0.9062 - acc: 0.7740\n",
      "Epoch 43/50\n",
      "13158/13158 [==============================] - 9s 652us/step - loss: 0.8902 - acc: 0.7756\n",
      "Epoch 44/50\n",
      "13158/13158 [==============================] - 9s 652us/step - loss: 0.8910 - acc: 0.7736\n",
      "Epoch 45/50\n",
      "13158/13158 [==============================] - 9s 653us/step - loss: 0.8737 - acc: 0.7765\n",
      "Epoch 46/50\n",
      "13158/13158 [==============================] - 9s 652us/step - loss: 0.8640 - acc: 0.7790\n",
      "Epoch 47/50\n",
      "13158/13158 [==============================] - 9s 661us/step - loss: 0.8430 - acc: 0.7816\n",
      "Epoch 48/50\n",
      "13158/13158 [==============================] - 9s 652us/step - loss: 0.8713 - acc: 0.7801\n",
      "Epoch 49/50\n",
      "13158/13158 [==============================] - 9s 664us/step - loss: 0.8537 - acc: 0.7810\n",
      "Epoch 50/50\n",
      "13158/13158 [==============================] - 9s 661us/step - loss: 0.8485 - acc: 0.7825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bdedbb8400>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7min 18s\n"
     ]
    }
   ],
   "source": [
    "cnn_model.fit(X_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=50,\n",
    "          verbose=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving all 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.98 s\n"
     ]
    }
   ],
   "source": [
    "# filename = 'savefiles/wordmaxent.sav'\n",
    "# pickle.dump(count_model, open(filename, 'wb'))\n",
    "\n",
    "# filename = 'savefiles/tagmaxent.sav'\n",
    "# pickle.dump(tag_model, open(filename, 'wb'))\n",
    "\n",
    "# filename = 'savefiles/CNNpredictor.sav'\n",
    "# pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 842 ms\n"
     ]
    }
   ],
   "source": [
    "count_model = pickle.load(open('savefiles/wordmaxent.sav','rb'))\n",
    "tag_model = pickle.load(open('savefiles/tagmaxent.sav', 'rb'))\n",
    "#cnn_model = pickle.load(open('savefiles/CNNpredictor.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.99 ms\n"
     ]
    }
   ],
   "source": [
    "def input_tag_to_prediction(inp):\n",
    "    words = nltk.word_tokenize(inp)\n",
    "    tags = [tag[1] for tag in nltk.pos_tag(words)]\n",
    "    integer_encoded = label_encoder.transform(tags)\n",
    "    X_encoded = onehot_encoder.transform(integer_encoded.reshape(len(integer_encoded), 1)).flatten()\n",
    "    possible_predictions = tag_model.predict_proba(X_encoded.reshape(1, -1))[0]\n",
    "    indices = (-possible_predictions).argsort()[:3]\n",
    "    return [label_encoder.inverse_transform([idx])[0] for idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 997 µs\n"
     ]
    }
   ],
   "source": [
    "def input_word_to_prediction(inp):\n",
    "    words = nltk.word_tokenize(inp)\n",
    "    words_tf = np.array(vectorizer.transform(words).toarray()).flatten()\n",
    "    possible_predictions = count_model.predict_proba(words_tf.reshape(1, -1))[0]\n",
    "    indices = (-possible_predictions).argsort()[:3]\n",
    "    return set(feats[idx] for idx in indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 998 µs\n"
     ]
    }
   ],
   "source": [
    "def prediction_union(sent):\n",
    "    sent = sent.lower()\n",
    "    pw, pt = input_word_to_prediction(sent), input_tag_to_prediction(sent)\n",
    "    return pw, pt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.99 ms\n"
     ]
    }
   ],
   "source": [
    "def DLpredict(sent):\n",
    "    words = nltk.word_tokenize(sent)\n",
    "    x = np.array(vectorizer.transform(words).toarray())\n",
    "    x = x.reshape(1,x.shape[0],x.shape[1],1)\n",
    "    y_pred = model.predict(x)[0]\n",
    "    indices = (-y_pred).argsort()[:3]\n",
    "    return [feats[index] for index in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 978 µs\n"
     ]
    }
   ],
   "source": [
    "def run(sent):\n",
    "    out = sent\n",
    "    for i in range(16):\n",
    "        print(out)\n",
    "        pw,pt = prediction_union(sent)\n",
    "        cw = DLpredict(sent)\n",
    "        print(\"predictions: \"+str(pw.union(cw))+\" -> model suggests a \"+str(tagmap[pt]))\n",
    "        inp = input()\n",
    "        if inp==\"exit\":\n",
    "            return out\n",
    "        out+= \" \" +inp\n",
    "        sent = \" \".join(nltk.word_tokenize(sent)[1:]+[inp])\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "where is ai\n",
      "predictions: {'in', 'might', 'research', 'to'} -> model suggests a noun, singular ex:‘desk\n",
      "research\n",
      "where is ai research\n",
      "predictions: {'in', 'was', 'as'} -> model suggests a preposition/subordinating conjunction\n",
      "heading\n",
      "where is ai research heading\n",
      "predictions: {'bird', 'to', 'drugs', 'the', 'and'} -> model suggests a noun, singular ex:‘desk\n",
      "towards\n",
      "where is ai research heading towards\n",
      "predictions: {'in', 'artificial', 'to', 'creating', 'the', 'u'} -> model suggests a verb, sing. present, non-3d ex:take\n"
     ]
    }
   ],
   "source": [
    "out = run(\"where is ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'artificial intelligence is the field of ai where the focus is not centered on carnegie mellon university'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.62 ms\n"
     ]
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix removal of punctuations\n",
    "#improve CNN accuracy and visualize layer contributions\n",
    "#integrate into iOS as python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
